<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
<head>
<META http-equiv="Content-Type" content="text/html; charset=UTF-8">
<!--
This HTML was auto-generated from MATLAB code.
To make changes, update the MATLAB code and republish this document.
      -->
<title>BandwidthRLAgent</title>
<meta name="generator" content="MATLAB 25.2">
<link rel="schema.DC" href="http://purl.org/dc/elements/1.1/">
<meta name="DC.date" content="2025-10-05">
<meta name="DC.source" content="BandwidthRLAgent.m">
<style type="text/css">
html,body,div,span,applet,object,iframe,h1,h2,h3,h4,h5,h6,p,blockquote,pre,a,abbr,acronym,address,big,cite,code,del,dfn,em,font,img,ins,kbd,q,s,samp,small,strike,strong,tt,var,b,u,i,center,dl,dt,dd,ol,ul,li,fieldset,form,label,legend,table,caption,tbody,tfoot,thead,tr,th,td{margin:0;padding:0;border:0;outline:0;font-size:100%;vertical-align:baseline;background:transparent}body{line-height:1}ol,ul{list-style:none}blockquote,q{quotes:none}blockquote:before,blockquote:after,q:before,q:after{content:'';content:none}:focus{outine:0}ins{text-decoration:none}del{text-decoration:line-through}table{border-collapse:collapse;border-spacing:0}

html { min-height:100%; margin-bottom:1px; }
html body { height:100%; margin:0px; font-family:Arial, Helvetica, sans-serif; font-size:10px; color:#000; line-height:140%; background:#fff none; overflow-y:scroll; }
html body td { vertical-align:top; text-align:left; }

h1 { padding:0px; margin:0px 0px 25px; font-family:Arial, Helvetica, sans-serif; font-size:1.5em; color:#d55000; line-height:100%; font-weight:normal; }
h2 { padding:0px; margin:0px 0px 8px; font-family:Arial, Helvetica, sans-serif; font-size:1.2em; color:#000; font-weight:bold; line-height:140%; border-bottom:1px solid #d6d4d4; display:block; }
h3 { padding:0px; margin:0px 0px 5px; font-family:Arial, Helvetica, sans-serif; font-size:1.1em; color:#000; font-weight:bold; line-height:140%; }

a { color:#005fce; text-decoration:none; }
a:hover { color:#005fce; text-decoration:underline; }
a:visited { color:#004aa0; text-decoration:none; }

p { padding:0px; margin:0px 0px 20px; }
img { padding:0px; margin:0px 0px 20px; border:none; }
p img, pre img, tt img, li img, h1 img, h2 img { margin-bottom:0px; }

ul { padding:0px; margin:0px 0px 20px 23px; list-style:square; }
ul li { padding:0px; margin:0px 0px 7px 0px; }
ul li ul { padding:5px 0px 0px; margin:0px 0px 7px 23px; }
ul li ol li { list-style:decimal; }
ol { padding:0px; margin:0px 0px 20px 0px; list-style:decimal; }
ol li { padding:0px; margin:0px 0px 7px 23px; list-style-type:decimal; }
ol li ol { padding:5px 0px 0px; margin:0px 0px 7px 0px; }
ol li ol li { list-style-type:lower-alpha; }
ol li ul { padding-top:7px; }
ol li ul li { list-style:square; }

.content { font-size:1.2em; line-height:140%; padding: 20px; }

pre, code { font-size:12px; }
tt { font-size: 1.2em; }
pre { margin:0px 0px 20px; }
pre.codeinput { padding:10px; border:1px solid #d3d3d3; background:#f7f7f7; }
pre.codeoutput { padding:10px 11px; margin:0px 0px 20px; color:#4c4c4c; }
pre.error { color:red; }

@media print { pre.codeinput, pre.codeoutput { word-wrap:break-word; width:100%; } }

span.keyword { color:#0000FF }
span.comment { color:#228B22 }
span.string { color:#A020F0 }
span.untermstring { color:#B20000 }
span.syscmd { color:#B28C00 }
span.typesection { color:#A0522D }

.footer { width:auto; padding:10px 0px; margin:25px 0px 0px; border-top:1px dotted #878787; font-size:0.8em; line-height:140%; font-style:italic; color:#878787; text-align:left; float:none; }
.footer p { margin:0px; }
.footer a { color:#878787; }
.footer a:hover { color:#878787; text-decoration:underline; }
.footer a:visited { color:#878787; }

table th { padding:7px 5px; text-align:left; vertical-align:middle; border: 1px solid #d6d4d4; font-weight:bold; }
table td { padding:7px 5px; text-align:left; vertical-align:top; border:1px solid #d6d4d4; }





  </style>
</head>
<body>
<div class="content">
<h2>Contents</h2>
<div>
<ul>
<li>
<a href="#3">Individual satisfaction rewards (symmetric for all traffic types)</a>
</li>
<li>
<a href="#4">CRITICAL: Large bonus for balanced performance</a>
</li>
<li>
<a href="#5">Penalty for severe imbalance</a>
</li>
<li>
<a href="#6">Moderate waste penalty</a>
</li>
<li>
<a href="#7">Small efficiency bonus</a>
</li>
</ul>
</div>
<pre class="codeinput">
<span class="keyword">classdef</span> BandwidthRLAgent &lt; handle
    <span class="comment">% Bandwidth Allocation RL Agent - FIXED VERSION</span>
    <span class="comment">% Implements strong starvation prevention with balanced rewards</span>

    <span class="keyword">properties</span>
        <span class="comment">% Q-learning parameters</span>
        learning_rate = 0.2;  <span class="comment">% Faster learning</span>
        discount_factor = 0.85;  <span class="comment">% Less focus on future rewards</span>
        exploration_rate = 0.6;  <span class="comment">% More exploration</span>
        exploration_decay = 0.998;  <span class="comment">% Very slow decay</span>
        min_exploration = 0.15;  <span class="comment">% Keep exploring longer</span>

        <span class="comment">% Q-table</span>
        Q_table;

        <span class="comment">% Enhanced action space with more protective allocations</span>
        action_space = [
            <span class="comment">% Balanced allocations</span>
            0.33, 0.33, 0.34;   <span class="comment">% Equal distribution</span>
            0.40, 0.30, 0.30;   <span class="comment">% Web focus</span>
            0.30, 0.40, 0.30;   <span class="comment">% Audio focus</span>
            0.30, 0.30, 0.40;   <span class="comment">% Video focus</span>

            <span class="comment">% Web-priority scenarios</span>
            0.50, 0.25, 0.25;   <span class="comment">% High web demand</span>
            0.45, 0.30, 0.25;   <span class="comment">% Moderate web priority</span>

            <span class="comment">% Audio-priority scenarios (protect real-time)</span>
            0.25, 0.50, 0.25;   <span class="comment">% High audio demand</span>
            0.30, 0.45, 0.25;   <span class="comment">% Moderate audio priority</span>

            <span class="comment">% Video-priority scenarios</span>
            0.25, 0.25, 0.50;   <span class="comment">% High video demand</span>
            0.30, 0.25, 0.45;   <span class="comment">% Moderate video priority</span>

            <span class="comment">% Balanced combinations</span>
            0.35, 0.35, 0.30;   <span class="comment">% Web+Audio balanced</span>
            0.35, 0.30, 0.35;   <span class="comment">% Web+Video balanced</span>
            0.30, 0.35, 0.35;   <span class="comment">% Audio+Video balanced</span>

            <span class="comment">% Protection allocations (prevent starvation)</span>
            0.40, 0.35, 0.25;   <span class="comment">% Web+Audio protect</span>
            0.25, 0.40, 0.35;   <span class="comment">% Audio+Video protect</span>
            0.35, 0.25, 0.40;   <span class="comment">% Web+Video protect</span>

            <span class="comment">% Emergency allocations</span>
            0.20, 0.40, 0.40;   <span class="comment">% Sacrifice web for others</span>
            0.40, 0.20, 0.40;   <span class="comment">% Sacrifice audio for others</span>
            0.40, 0.40, 0.20;   <span class="comment">% Sacrifice video for others</span>
        ];

        <span class="comment">% Training history</span>
        training_history = [];
        episode_count = 0;
    <span class="keyword">end</span>

    <span class="keyword">methods</span>
        <span class="keyword">function</span> obj = BandwidthRLAgent()
            <span class="comment">% Initialize Q-table</span>
            num_states = 3 * 4 * 4 * 6;  <span class="comment">% 288 states</span>
            num_actions = size(obj.action_space, 1);
            obj.Q_table = zeros(num_states, num_actions);
        <span class="keyword">end</span>

        <span class="keyword">function</span> state_index = discretize_state(obj, web_users, audio_users, video_users, <span class="keyword">...</span>
                                              web_demand, audio_demand, video_demand, <span class="keyword">...</span>
                                              web_sat, audio_sat, video_sat, total_demand)

            <span class="comment">% 1. Dominant traffic type (which service has most users)</span>
            total_users = web_users + audio_users + video_users;
            <span class="keyword">if</span> total_users &gt; 0
                web_ratio = web_users / total_users;
                audio_ratio = audio_users / total_users;
                video_ratio = video_users / total_users;
                [~, dominant_type] = max([web_ratio, audio_ratio, video_ratio]);
            <span class="keyword">else</span>
                dominant_type = 1;
            <span class="keyword">end</span>

            <span class="comment">% 2. Congestion level (network utilization)</span>
            congestion_level = total_demand / 100;
            <span class="keyword">if</span> congestion_level &lt; 0.7
                congestion_idx = 1;      <span class="comment">% Light load</span>
            <span class="keyword">elseif</span> congestion_level &lt; 0.9
                congestion_idx = 2;      <span class="comment">% Moderate load</span>
            <span class="keyword">elseif</span> congestion_level &lt; 1.0
                congestion_idx = 3;      <span class="comment">% Near capacity</span>
            <span class="keyword">else</span>
                congestion_idx = 4;      <span class="comment">% Overloaded</span>
            <span class="keyword">end</span>

            <span class="comment">% 3. Starvation risk (how many services are struggling)</span>
            starvation_count = sum([web_sat &lt; 60 &amp;&amp; web_users &gt; 0, <span class="keyword">...</span>
                                   audio_sat &lt; 60 &amp;&amp; audio_users &gt; 0, <span class="keyword">...</span>
                                   video_sat &lt; 60 &amp;&amp; video_users &gt; 0]);
            starvation_count = min(starvation_count, 3);

            <span class="comment">% 4. Worst satisfaction level</span>
            min_sat = min([web_sat, audio_sat, video_sat]);
            sat_bins = [0, 30, 50, 70, 85, 100];
            sat_idx = discretize(min_sat, sat_bins);
            <span class="keyword">if</span> isnan(sat_idx), sat_idx = length(sat_bins); <span class="keyword">end</span>

            <span class="comment">% Combine features into state index</span>
            state_index = (dominant_type-1) * 4 * 4 * 6 + <span class="keyword">...</span>
                         (congestion_idx-1) * 4 * 6 + <span class="keyword">...</span>
                         (starvation_count) * 6 + <span class="keyword">...</span>
                         sat_idx;

            state_index = min(max(state_index, 1), 288);
        <span class="keyword">end</span>

        <span class="keyword">function</span> [web_ratio, audio_ratio, video_ratio] = predict(obj, state)
            <span class="comment">% Choose action using epsilon-greedy policy</span>

            state_idx = obj.discretize_state(<span class="keyword">...</span>
                state.web_users, state.audio_users, state.video_users, <span class="keyword">...</span>
                state.web_demand, state.audio_demand, state.video_demand, <span class="keyword">...</span>
                state.web_sat, state.audio_sat, state.video_sat, <span class="keyword">...</span>
                state.total_demand);

            state_idx = min(max(state_idx, 1), size(obj.Q_table, 1));

            <span class="comment">% Epsilon-greedy action selection</span>
            <span class="keyword">if</span> rand() &lt; obj.exploration_rate
                <span class="comment">% Explore: random action</span>
                action_idx = randi(size(obj.action_space, 1));
            <span class="keyword">else</span>
                <span class="comment">% Exploit: best known action</span>
                [~, action_idx] = max(obj.Q_table(state_idx, :));
            <span class="keyword">end</span>

            action = obj.action_space(action_idx, :);
            web_ratio = action(1);
            audio_ratio = action(2);
            video_ratio = action(3);
        <span class="keyword">end</span>

        <span class="keyword">function</span> reward = calculate_reward(obj, state, action, next_state)
</pre>
<pre class="codeinput">            <span class="comment">% GRADUAL REWARD FUNCTION - Provides clear learning gradients</span>

            reward = 0;
</pre>
<h2 id="3">Individual satisfaction rewards (symmetric for all traffic types)</h2>
<p>Use gradual rewards instead of harsh thresholds</p>
<pre class="codeinput">            <span class="comment">% Web satisfaction</span>
            <span class="keyword">if</span> state.web_users &gt; 0
                <span class="keyword">if</span> next_state.web_sat &gt;= 80
                    reward = reward + 3.0;
                <span class="keyword">elseif</span> next_state.web_sat &gt;= 70
                    reward = reward + 2.0;
                <span class="keyword">elseif</span> next_state.web_sat &gt;= 60
                    reward = reward + 1.0;
                <span class="keyword">elseif</span> next_state.web_sat &gt;= 50
                    reward = reward + 0.5;
                <span class="keyword">elseif</span> next_state.web_sat &gt;= 40
                    reward = reward - 2.0;
                <span class="keyword">elseif</span> next_state.web_sat &gt;= 30
                    reward = reward - 5.0;
                <span class="keyword">else</span>
                    reward = reward - 8.0;
                <span class="keyword">end</span>
            <span class="keyword">end</span>

            <span class="comment">% Audio satisfaction (same scale)</span>
            <span class="keyword">if</span> state.audio_users &gt; 0
                <span class="keyword">if</span> next_state.audio_sat &gt;= 80
                    reward = reward + 3.0;
                <span class="keyword">elseif</span> next_state.audio_sat &gt;= 70
                    reward = reward + 2.0;
                <span class="keyword">elseif</span> next_state.audio_sat &gt;= 60
                    reward = reward + 1.0;
                <span class="keyword">elseif</span> next_state.audio_sat &gt;= 50
                    reward = reward + 0.5;
                <span class="keyword">elseif</span> next_state.audio_sat &gt;= 40
                    reward = reward - 2.0;
                <span class="keyword">elseif</span> next_state.audio_sat &gt;= 30
                    reward = reward - 5.0;
                <span class="keyword">else</span>
                    reward = reward - 8.0;
                <span class="keyword">end</span>
            <span class="keyword">end</span>

            <span class="comment">% Video satisfaction (same scale - equal treatment)</span>
            <span class="keyword">if</span> state.video_users &gt; 0
                <span class="keyword">if</span> next_state.video_sat &gt;= 80
                    reward = reward + 3.0;
                <span class="keyword">elseif</span> next_state.video_sat &gt;= 70
                    reward = reward + 2.0;
                <span class="keyword">elseif</span> next_state.video_sat &gt;= 60
                    reward = reward + 1.0;
                <span class="keyword">elseif</span> next_state.video_sat &gt;= 50
                    reward = reward + 0.5;
                <span class="keyword">elseif</span> next_state.video_sat &gt;= 40
                    reward = reward - 2.0;
                <span class="keyword">elseif</span> next_state.video_sat &gt;= 30
                    reward = reward - 5.0;
                <span class="keyword">else</span>
                    reward = reward - 8.0;
                <span class="keyword">end</span>
            <span class="keyword">end</span>
</pre>
<h2 id="4">CRITICAL: Large bonus for balanced performance</h2>
<p>This is the key to preventing one service from being sacrificed</p>
<pre class="codeinput">            min_satisfaction = min([next_state.web_sat, next_state.audio_sat, next_state.video_sat]);

            <span class="keyword">if</span> min_satisfaction &gt;= 70
                reward = reward + 10.0;  <span class="comment">% Big bonus</span>
            <span class="keyword">elseif</span> min_satisfaction &gt;= 60
                reward = reward + 5.0;
            <span class="keyword">elseif</span> min_satisfaction &gt;= 50
                reward = reward + 2.0;
            <span class="keyword">end</span>
</pre>
<h2 id="5">Penalty for severe imbalance</h2>
<pre class="codeinput">            sat_values = [next_state.web_sat, next_state.audio_sat, next_state.video_sat];
            sat_std = std(sat_values);

            <span class="keyword">if</span> sat_std &gt; 30  <span class="comment">% High variance = unfair</span>
                reward = reward - 3.0;
            <span class="keyword">end</span>
</pre>
<h2 id="6">Moderate waste penalty</h2>
<pre class="codeinput">            <span class="keyword">if</span> next_state.web_sat &gt; 120
                reward = reward - (next_state.web_sat - 120) / 50;
            <span class="keyword">end</span>
            <span class="keyword">if</span> next_state.audio_sat &gt; 120
                reward = reward - (next_state.audio_sat - 120) / 50;
            <span class="keyword">end</span>
            <span class="keyword">if</span> next_state.video_sat &gt; 120
                reward = reward - (next_state.video_sat - 120) / 50;
            <span class="keyword">end</span>
</pre>
<h2 id="7">Small efficiency bonus</h2>
<pre class="codeinput">            utilization = state.total_demand / 100;
            <span class="keyword">if</span> utilization &gt; 0.7 &amp;&amp; utilization &lt; 0.95
                reward = reward + 1.0;
            <span class="keyword">end</span>

            <span class="comment">% Wide range for clear gradients</span>
            reward = max(-25, min(25, reward));
</pre>
<pre class="codeinput">        <span class="keyword">end</span>

        <span class="keyword">function</span> update(obj, state, action, reward, next_state)
            <span class="comment">% Q-learning update rule</span>

            state_idx = obj.discretize_state(<span class="keyword">...</span>
                state.web_users, state.audio_users, state.video_users, <span class="keyword">...</span>
                state.web_demand, state.audio_demand, state.video_demand, <span class="keyword">...</span>
                state.web_sat, state.audio_sat, state.video_sat, <span class="keyword">...</span>
                state.total_demand);

            next_state_idx = obj.discretize_state(<span class="keyword">...</span>
                next_state.web_users, next_state.audio_users, next_state.video_users, <span class="keyword">...</span>
                next_state.web_demand, next_state.audio_demand, next_state.video_demand, <span class="keyword">...</span>
                next_state.web_sat, next_state.audio_sat, next_state.video_sat, <span class="keyword">...</span>
                next_state.total_demand);

            state_idx = min(max(state_idx, 1), size(obj.Q_table, 1));
            next_state_idx = min(max(next_state_idx, 1), size(obj.Q_table, 1));

            <span class="comment">% Find action index</span>
            action_vec = [action.web_ratio, action.audio_ratio, action.video_ratio];
            [~, action_idx] = min(vecnorm(obj.action_space - action_vec, 2, 2));

            <span class="comment">% Q-learning update: Q(s,a) = Q(s,a) + α[r + γ*max(Q(s',a')) - Q(s,a)]</span>
            current_q = obj.Q_table(state_idx, action_idx);
            max_next_q = max(obj.Q_table(next_state_idx, :));

            obj.Q_table(state_idx, action_idx) = current_q + <span class="keyword">...</span>
                obj.learning_rate * (reward + obj.discount_factor * max_next_q - current_q);

            <span class="comment">% Decay exploration rate</span>
            obj.exploration_rate = max(obj.min_exploration, <span class="keyword">...</span>
                obj.exploration_rate * obj.exploration_decay);

            <span class="comment">% Store training history</span>
            obj.episode_count = obj.episode_count + 1;
            history_entry = struct(<span class="keyword">...</span>
                <span class="string">'episode'</span>, obj.episode_count, <span class="keyword">...</span>
                <span class="string">'state'</span>, state, <span class="keyword">...</span>
                <span class="string">'action'</span>, action_vec, <span class="keyword">...</span>
                <span class="string">'reward'</span>, reward, <span class="keyword">...</span>
                <span class="string">'next_state'</span>, next_state, <span class="keyword">...</span>
                <span class="string">'exploration_rate'</span>, obj.exploration_rate);

            obj.training_history = [obj.training_history; history_entry];
        <span class="keyword">end</span>

        <span class="keyword">function</span> performance = evaluate_performance(obj)
            <span class="comment">% Evaluate agent performance over recent episodes</span>

            <span class="keyword">if</span> isempty(obj.training_history)
                performance = struct();
                <span class="keyword">return</span>;
            <span class="keyword">end</span>

            recent_episodes = max(1, length(obj.training_history)-99):length(obj.training_history);
            recent_rewards = [obj.training_history(recent_episodes).reward];

            recent_states = [obj.training_history(recent_episodes).next_state];
            <span class="keyword">if</span> ~isempty(recent_states)
                web_sats = [recent_states.web_sat];
                audio_sats = [recent_states.audio_sat];
                video_sats = [recent_states.video_sat];
            <span class="keyword">else</span>
                web_sats = 0; audio_sats = 0; video_sats = 0;
            <span class="keyword">end</span>

            performance = struct(<span class="keyword">...</span>
                <span class="string">'average_reward'</span>, mean(recent_rewards), <span class="keyword">...</span>
                <span class="string">'std_reward'</span>, std(recent_rewards), <span class="keyword">...</span>
                <span class="string">'min_reward'</span>, min(recent_rewards), <span class="keyword">...</span>
                <span class="string">'max_reward'</span>, max(recent_rewards), <span class="keyword">...</span>
                <span class="string">'total_episodes'</span>, obj.episode_count, <span class="keyword">...</span>
                <span class="string">'current_exploration'</span>, obj.exploration_rate, <span class="keyword">...</span>
                <span class="string">'q_table_size'</span>, size(obj.Q_table), <span class="keyword">...</span>
                <span class="string">'action_space_size'</span>, size(obj.action_space, 1), <span class="keyword">...</span>
                <span class="string">'avg_web_sat'</span>, mean(web_sats), <span class="keyword">...</span>
                <span class="string">'avg_audio_sat'</span>, mean(audio_sats), <span class="keyword">...</span>
                <span class="string">'avg_video_sat'</span>, mean(video_sats));
        <span class="keyword">end</span>

        <span class="keyword">function</span> print_policy_analysis(obj)
            <span class="comment">% Print detailed policy analysis</span>
            fprintf(<span class="string">'\n=== RL AGENT POLICY ANALYSIS ===\n'</span>);
            fprintf(<span class="string">'Total training episodes: %d\n'</span>, obj.episode_count);
            fprintf(<span class="string">'Exploration rate: %.3f\n'</span>, obj.exploration_rate);
            fprintf(<span class="string">'Q-table size: %d states x %d actions\n'</span>, size(obj.Q_table, 1), size(obj.Q_table, 2));

            <span class="keyword">if</span> ~isempty(obj.training_history)
                recent_episodes = max(1, length(obj.training_history)-50):length(obj.training_history);
                recent_rewards = [obj.training_history(recent_episodes).reward];

                fprintf(<span class="string">'Recent average reward: %.2f\n'</span>, mean(recent_rewards));
                fprintf(<span class="string">'Recent reward std: %.2f\n'</span>, std(recent_rewards));

                <span class="comment">% Action usage analysis</span>
                action_counts = zeros(size(obj.action_space, 1), 1);
                action_rewards = zeros(size(obj.action_space, 1), 1);

                <span class="keyword">for</span> i = 1:length(recent_episodes)
                    action_vec = obj.training_history(recent_episodes(i)).action;
                    [~, action_idx] = min(vecnorm(obj.action_space - action_vec, 2, 2));
                    action_counts(action_idx) = action_counts(action_idx) + 1;
                    action_rewards(action_idx) = action_rewards(action_idx) + recent_rewards(i);
                <span class="keyword">end</span>

                action_avg_rewards = action_rewards ./ max(1, action_counts);

                [~, top_actions] = sort(action_counts, <span class="string">'descend'</span>);
                fprintf(<span class="string">'\nTop 5 most used actions:\n'</span>);
                <span class="keyword">for</span> i = 1:min(5, length(top_actions))
                    <span class="keyword">if</span> action_counts(top_actions(i)) &gt; 0
                        action = obj.action_space(top_actions(i), :);
                        fprintf(<span class="string">'  Action %d: Web=%.0f%%, Audio=%.0f%%, Video=%.0f%% (used %d times, avg reward: %.2f)\n'</span>, <span class="keyword">...</span>
                            top_actions(i), action(1)*100, action(2)*100, action(3)*100, <span class="keyword">...</span>
                            action_counts(top_actions(i)), action_avg_rewards(top_actions(i)));
                    <span class="keyword">end</span>
                <span class="keyword">end</span>
            <span class="keyword">end</span>
        <span class="keyword">end</span>
    <span class="keyword">end</span>
<span class="keyword">end</span>
</pre>
<pre class="codeoutput">
ans = 

  BandwidthRLAgent with properties:

        learning_rate: 0.2000
      discount_factor: 0.8500
     exploration_rate: 0.6000
    exploration_decay: 0.9980
      min_exploration: 0.1500
              Q_table: [288&times;19 double]
         action_space: [19&times;3 double]
     training_history: []
        episode_count: 0

</pre>
<p class="footer">
<br>
<a href="https://www.mathworks.com/products/matlab/">Published with MATLAB&reg; R2025b</a>
<br>
</p>
</div>
<!--
##### SOURCE BEGIN #####
classdef BandwidthRLAgent < handle
    % Bandwidth Allocation RL Agent - FIXED VERSION
    % Implements strong starvation prevention with balanced rewards
    
    properties
        % Q-learning parameters
        learning_rate = 0.2;  % Faster learning
        discount_factor = 0.85;  % Less focus on future rewards
        exploration_rate = 0.6;  % More exploration
        exploration_decay = 0.998;  % Very slow decay
        min_exploration = 0.15;  % Keep exploring longer
        
        % Q-table
        Q_table;
        
        % Enhanced action space with more protective allocations
        action_space = [
            % Balanced allocations
            0.33, 0.33, 0.34;   % Equal distribution
            0.40, 0.30, 0.30;   % Web focus
            0.30, 0.40, 0.30;   % Audio focus
            0.30, 0.30, 0.40;   % Video focus
            
            % Web-priority scenarios
            0.50, 0.25, 0.25;   % High web demand
            0.45, 0.30, 0.25;   % Moderate web priority
            
            % Audio-priority scenarios (protect real-time)
            0.25, 0.50, 0.25;   % High audio demand
            0.30, 0.45, 0.25;   % Moderate audio priority
            
            % Video-priority scenarios
            0.25, 0.25, 0.50;   % High video demand
            0.30, 0.25, 0.45;   % Moderate video priority
            
            % Balanced combinations
            0.35, 0.35, 0.30;   % Web+Audio balanced
            0.35, 0.30, 0.35;   % Web+Video balanced
            0.30, 0.35, 0.35;   % Audio+Video balanced
            
            % Protection allocations (prevent starvation)
            0.40, 0.35, 0.25;   % Web+Audio protect
            0.25, 0.40, 0.35;   % Audio+Video protect
            0.35, 0.25, 0.40;   % Web+Video protect
            
            % Emergency allocations
            0.20, 0.40, 0.40;   % Sacrifice web for others
            0.40, 0.20, 0.40;   % Sacrifice audio for others
            0.40, 0.40, 0.20;   % Sacrifice video for others
        ];
        
        % Training history
        training_history = [];
        episode_count = 0;
    end
    
    methods
        function obj = BandwidthRLAgent()
            % Initialize Q-table
            num_states = 3 * 4 * 4 * 6;  % 288 states
            num_actions = size(obj.action_space, 1);
            obj.Q_table = zeros(num_states, num_actions);
        end
        
        function state_index = discretize_state(obj, web_users, audio_users, video_users, ...
                                              web_demand, audio_demand, video_demand, ...
                                              web_sat, audio_sat, video_sat, total_demand)
            
            % 1. Dominant traffic type (which service has most users)
            total_users = web_users + audio_users + video_users;
            if total_users > 0
                web_ratio = web_users / total_users;
                audio_ratio = audio_users / total_users;
                video_ratio = video_users / total_users;
                [~, dominant_type] = max([web_ratio, audio_ratio, video_ratio]);
            else
                dominant_type = 1;
            end
            
            % 2. Congestion level (network utilization)
            congestion_level = total_demand / 100;
            if congestion_level < 0.7
                congestion_idx = 1;      % Light load
            elseif congestion_level < 0.9
                congestion_idx = 2;      % Moderate load
            elseif congestion_level < 1.0
                congestion_idx = 3;      % Near capacity
            else
                congestion_idx = 4;      % Overloaded
            end
            
            % 3. Starvation risk (how many services are struggling)
            starvation_count = sum([web_sat < 60 && web_users > 0, ...
                                   audio_sat < 60 && audio_users > 0, ...
                                   video_sat < 60 && video_users > 0]);
            starvation_count = min(starvation_count, 3);
            
            % 4. Worst satisfaction level
            min_sat = min([web_sat, audio_sat, video_sat]);
            sat_bins = [0, 30, 50, 70, 85, 100];
            sat_idx = discretize(min_sat, sat_bins);
            if isnan(sat_idx), sat_idx = length(sat_bins); end
            
            % Combine features into state index
            state_index = (dominant_type-1) * 4 * 4 * 6 + ...
                         (congestion_idx-1) * 4 * 6 + ...
                         (starvation_count) * 6 + ...
                         sat_idx;
            
            state_index = min(max(state_index, 1), 288);
        end
        
        function [web_ratio, audio_ratio, video_ratio] = predict(obj, state)
            % Choose action using epsilon-greedy policy
            
            state_idx = obj.discretize_state(...
                state.web_users, state.audio_users, state.video_users, ...
                state.web_demand, state.audio_demand, state.video_demand, ...
                state.web_sat, state.audio_sat, state.video_sat, ...
                state.total_demand);
            
            state_idx = min(max(state_idx, 1), size(obj.Q_table, 1));
            
            % Epsilon-greedy action selection
            if rand() < obj.exploration_rate
                % Explore: random action
                action_idx = randi(size(obj.action_space, 1));
            else
                % Exploit: best known action
                [~, action_idx] = max(obj.Q_table(state_idx, :));
            end
            
            action = obj.action_space(action_idx, :);
            web_ratio = action(1);
            audio_ratio = action(2);
            video_ratio = action(3);
        end
        
        function reward = calculate_reward(obj, state, action, next_state)
            % GRADUAL REWARD FUNCTION - Provides clear learning gradients
            
            reward = 0;
            
            %% Individual satisfaction rewards (symmetric for all traffic types)
            % Use gradual rewards instead of harsh thresholds
            
            % Web satisfaction
            if state.web_users > 0
                if next_state.web_sat >= 80
                    reward = reward + 3.0;
                elseif next_state.web_sat >= 70
                    reward = reward + 2.0;
                elseif next_state.web_sat >= 60
                    reward = reward + 1.0;
                elseif next_state.web_sat >= 50
                    reward = reward + 0.5;
                elseif next_state.web_sat >= 40
                    reward = reward - 2.0;
                elseif next_state.web_sat >= 30
                    reward = reward - 5.0;
                else
                    reward = reward - 8.0;
                end
            end
            
            % Audio satisfaction (same scale)
            if state.audio_users > 0
                if next_state.audio_sat >= 80
                    reward = reward + 3.0;
                elseif next_state.audio_sat >= 70
                    reward = reward + 2.0;
                elseif next_state.audio_sat >= 60
                    reward = reward + 1.0;
                elseif next_state.audio_sat >= 50
                    reward = reward + 0.5;
                elseif next_state.audio_sat >= 40
                    reward = reward - 2.0;
                elseif next_state.audio_sat >= 30
                    reward = reward - 5.0;
                else
                    reward = reward - 8.0;
                end
            end
            
            % Video satisfaction (same scale - equal treatment)
            if state.video_users > 0
                if next_state.video_sat >= 80
                    reward = reward + 3.0;
                elseif next_state.video_sat >= 70
                    reward = reward + 2.0;
                elseif next_state.video_sat >= 60
                    reward = reward + 1.0;
                elseif next_state.video_sat >= 50
                    reward = reward + 0.5;
                elseif next_state.video_sat >= 40
                    reward = reward - 2.0;
                elseif next_state.video_sat >= 30
                    reward = reward - 5.0;
                else
                    reward = reward - 8.0;
                end
            end
            
            %% CRITICAL: Large bonus for balanced performance
            % This is the key to preventing one service from being sacrificed
            min_satisfaction = min([next_state.web_sat, next_state.audio_sat, next_state.video_sat]);
            
            if min_satisfaction >= 70
                reward = reward + 10.0;  % Big bonus
            elseif min_satisfaction >= 60
                reward = reward + 5.0;
            elseif min_satisfaction >= 50
                reward = reward + 2.0;
            end
            
            %% Penalty for severe imbalance
            sat_values = [next_state.web_sat, next_state.audio_sat, next_state.video_sat];
            sat_std = std(sat_values);
            
            if sat_std > 30  % High variance = unfair
                reward = reward - 3.0;
            end
            
            %% Moderate waste penalty
            if next_state.web_sat > 120
                reward = reward - (next_state.web_sat - 120) / 50;
            end
            if next_state.audio_sat > 120
                reward = reward - (next_state.audio_sat - 120) / 50;
            end
            if next_state.video_sat > 120
                reward = reward - (next_state.video_sat - 120) / 50;
            end
            
            %% Small efficiency bonus
            utilization = state.total_demand / 100;
            if utilization > 0.7 && utilization < 0.95
                reward = reward + 1.0;
            end
            
            % Wide range for clear gradients
            reward = max(-25, min(25, reward));
        end
        
        function update(obj, state, action, reward, next_state)
            % Q-learning update rule
            
            state_idx = obj.discretize_state(...
                state.web_users, state.audio_users, state.video_users, ...
                state.web_demand, state.audio_demand, state.video_demand, ...
                state.web_sat, state.audio_sat, state.video_sat, ...
                state.total_demand);
            
            next_state_idx = obj.discretize_state(...
                next_state.web_users, next_state.audio_users, next_state.video_users, ...
                next_state.web_demand, next_state.audio_demand, next_state.video_demand, ...
                next_state.web_sat, next_state.audio_sat, next_state.video_sat, ...
                next_state.total_demand);
            
            state_idx = min(max(state_idx, 1), size(obj.Q_table, 1));
            next_state_idx = min(max(next_state_idx, 1), size(obj.Q_table, 1));
            
            % Find action index
            action_vec = [action.web_ratio, action.audio_ratio, action.video_ratio];
            [~, action_idx] = min(vecnorm(obj.action_space - action_vec, 2, 2));
            
            % Q-learning update: Q(s,a) = Q(s,a) + α[r + γ*max(Q(s',a')) - Q(s,a)]
            current_q = obj.Q_table(state_idx, action_idx);
            max_next_q = max(obj.Q_table(next_state_idx, :));
            
            obj.Q_table(state_idx, action_idx) = current_q + ...
                obj.learning_rate * (reward + obj.discount_factor * max_next_q - current_q);
            
            % Decay exploration rate
            obj.exploration_rate = max(obj.min_exploration, ...
                obj.exploration_rate * obj.exploration_decay);
            
            % Store training history
            obj.episode_count = obj.episode_count + 1;
            history_entry = struct(...
                'episode', obj.episode_count, ...
                'state', state, ...
                'action', action_vec, ...
                'reward', reward, ...
                'next_state', next_state, ...
                'exploration_rate', obj.exploration_rate);
            
            obj.training_history = [obj.training_history; history_entry];
        end
        
        function performance = evaluate_performance(obj)
            % Evaluate agent performance over recent episodes
            
            if isempty(obj.training_history)
                performance = struct();
                return;
            end
            
            recent_episodes = max(1, length(obj.training_history)-99):length(obj.training_history);
            recent_rewards = [obj.training_history(recent_episodes).reward];
            
            recent_states = [obj.training_history(recent_episodes).next_state];
            if ~isempty(recent_states)
                web_sats = [recent_states.web_sat];
                audio_sats = [recent_states.audio_sat];
                video_sats = [recent_states.video_sat];
            else
                web_sats = 0; audio_sats = 0; video_sats = 0;
            end
            
            performance = struct(...
                'average_reward', mean(recent_rewards), ...
                'std_reward', std(recent_rewards), ...
                'min_reward', min(recent_rewards), ...
                'max_reward', max(recent_rewards), ...
                'total_episodes', obj.episode_count, ...
                'current_exploration', obj.exploration_rate, ...
                'q_table_size', size(obj.Q_table), ...
                'action_space_size', size(obj.action_space, 1), ...
                'avg_web_sat', mean(web_sats), ...
                'avg_audio_sat', mean(audio_sats), ...
                'avg_video_sat', mean(video_sats));
        end
        
        function print_policy_analysis(obj)
            % Print detailed policy analysis
            fprintf('\n=== RL AGENT POLICY ANALYSIS ===\n');
            fprintf('Total training episodes: %d\n', obj.episode_count);
            fprintf('Exploration rate: %.3f\n', obj.exploration_rate);
            fprintf('Q-table size: %d states x %d actions\n', size(obj.Q_table, 1), size(obj.Q_table, 2));
            
            if ~isempty(obj.training_history)
                recent_episodes = max(1, length(obj.training_history)-50):length(obj.training_history);
                recent_rewards = [obj.training_history(recent_episodes).reward];
                
                fprintf('Recent average reward: %.2f\n', mean(recent_rewards));
                fprintf('Recent reward std: %.2f\n', std(recent_rewards));
                
                % Action usage analysis
                action_counts = zeros(size(obj.action_space, 1), 1);
                action_rewards = zeros(size(obj.action_space, 1), 1);
                
                for i = 1:length(recent_episodes)
                    action_vec = obj.training_history(recent_episodes(i)).action;
                    [~, action_idx] = min(vecnorm(obj.action_space - action_vec, 2, 2));
                    action_counts(action_idx) = action_counts(action_idx) + 1;
                    action_rewards(action_idx) = action_rewards(action_idx) + recent_rewards(i);
                end
                
                action_avg_rewards = action_rewards ./ max(1, action_counts);
                
                [~, top_actions] = sort(action_counts, 'descend');
                fprintf('\nTop 5 most used actions:\n');
                for i = 1:min(5, length(top_actions))
                    if action_counts(top_actions(i)) > 0
                        action = obj.action_space(top_actions(i), :);
                        fprintf('  Action %d: Web=%.0f%%, Audio=%.0f%%, Video=%.0f%% (used %d times, avg reward: %.2f)\n', ...
                            top_actions(i), action(1)*100, action(2)*100, action(3)*100, ...
                            action_counts(top_actions(i)), action_avg_rewards(top_actions(i)));
                    end
                end
            end
        end
    end
end
##### SOURCE END #####
-->
</body>
</html>
